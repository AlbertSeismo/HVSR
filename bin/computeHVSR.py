#!/usr/bin/env python
#
#
################################################################################################
#
# NAME: computeHVSR.py - a Python script that uses IRIS DMC's MUSTANG noise-psd/pdf web services
#       to compute horizontal-to-vertical spectral ratio (HVSR)
#
# Copyright (C) 2018  Product Team, IRIS Data Management Center
#
#    This is a free software; you can redistribute it and/or modify
#    it under the terms of the GNU Lesser General Public License as
#    published by the Free Software Foundation; either version 3 of the
#    License, or (at your option) any later version.
#
#    This script is distributed in the hope that it will be useful, but
#    WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
#    Lesser General Public License (GNU-LGPL) for more details.  The
#    GNU-LGPL and further information can be found here:
#    http://www.gnu.org/
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#
# INPUTS:
#    Script expects a configuration parameter file under the "param" directory. 
#    the parameter file's name is the same as this scripts name with "_param" appended.
#    Script also expects a station baseline file as generated by the getStationChannelBaseline.py
#    script.
#
# OUTPUTS:
#    - HVSR values
#    - HVSR plots
#    - SESAME 2004 parameters
#
# USAGE:
#              network station location channel list     start date       end date  plot(1/0) plot      plot accepted     verbose    y-axis x-axis type     break start-end     remove PSDs that fall
#                  |       |       |    |                |                 |             | rejected     accepted    plot  output(1/0)  max      |           interval into "n"   outside the station
#                  |       |       |    |                |                 |             |   PSDs(1/0)   PSDs(1/0)  PDFs(1/0) |         |       |              | segments       noise baseline
#                  |       |       |    |                |                 |             |       |         |        |         |         |       |              |                  |
#                  |       |       |    |                |                 |             |       |         |        |         |         |       |              |                  |      compute H/V using method (see below)
#                  |       |       |    |                |                 |             |       |         |        |         |         |       |              |                  |        |
#   computeHVSR.py net=TA sta=TCOL loc= chan=BHZ,BHN,BHE start=2013-01-01 end=2013-01-01 plot=1 plotbad=0 plotpsd=0 plotpdf=1 verbose=1 ymax=5 xtype=frequency n=1 removeoutliers=1 method=4
#
#             (1) DFA, Diffuse Field Assumption method (Sánchez-Sesma et al., 2011)
#                 NOTE: The MUSTANG noise-psd web service Power Spectral Density estimate for seismic channels are computed using the algorithm outlined here
#                       (http://service.iris.edu/mustang/noise-psd/docs/1/help/). This algorithm involves averaging and normalization that may result in
#                       smoothing of some of the peaks that may otherwise be observed by direct computation of FFT and DFA. With this smoothing the DFA
#                       results tend to be closer to the vector summation method, method (4) below.
#
#             NOTE: methods 2-6 are referenced by Albarello and Lunedei (2013)
#             (2) arithmetic mean, H ≡ (HN + HE)/2
#             (3) geometric mean, H ≡ √HN · HE
#             (4) vector summation, H ≡ √H2 N + H2 E
#             (5) quadratic mean, H ≡ √(H2 N + H2 E )/2
#             (6) maximum horizontal value, H ≡ max {HN, HE}
#
#
# HISTORY:
#
version = "R.2018191"
#
#    2018-07-10 IRIS DMC Product Team (Manoch): prerelease version R.2018191
#    2018-06-18 IRIS DMC Product Team (Manoch): added removeOutliers parameter to allow HVSR computation without removing outliers and added method parameter that 
#                                               the method to use for combining h1 and h2, including DFA R.2018169
#    2017-11-28 IRIS DMC Product Team (Manoch): R.2017332
#    2017-05-21 IRIS DMC Product Team (Manoch): R.2017141
#    2017-03-12 IRIS DMC Product Team (Manoch): created R.2017071
#
# REFERENCE:
#   Albarello, Dario & Lunedei, Enrico. (2013). Combining horizontal ambient vibration components for H/V spectral ratio estimates. Geophysical Journal International. 194. 936-951. 10.1093/gji/ggt130.
#   
#   Francisco J Sánchez-Sesma, Francisco & Rodriguez, Miguel & Iturraran-Viveros, Ursula & Luzón, Francisco & Campillo, Michel & Margerin, Ludovic & García-Jerez, Antonio & Suarez, Martha & Santoyo, Miguel &
#   Rodríguez-Castellanos, A. (2011). A theory for microtremor H/V spectral ratio: Application for a layered medium. Geophysical Journal International. 186. 221-225. 10.1111/j.1365-246X.2011.05064.x.
#   
#   Guidelines for the Implementation of the H/V Spectral Ratio Technique on Ambient Vibrations, December 2004
#   SESAME European research project WP12 – Deliverable D23.12, European Commission – Research General Directorate
#   Project No. EVG1-CT-2000-00026 SESAME.
#   ftp://ftp.geo.uib.no/pub/seismo/SOFTWARE/SESAME/USER-GUIDELINES/SESAME-HV-User-Guidelines.pdf
#
################################################################################################
#
# parameters to set initially
#
################################################################################################
import operator
import numpy as np
import time

GREEK_CHAR = {'sigma':u"\u03C3",'epsilon':u"\u03B5",'teta':  u"\u03B8"}
CHANNEL_ORDER = {'Z':0,'1':1,'N':1,'2':2,'E':2}
SEPARATOR_CHAR = '='

t0 = time.time()
display = True
maxRank = 0


def usage():
   """computeHVSR usage""" 
   print ("\n\nUSAGE("+version+"):\n\n")

   print ("              network station location channel list     start date       end date  plot(1/0) plot      plot accepted     verbose    y-axis x-axis type     break start-end     remove PSDs that fall")
   print ("                  |       |       |    |                |                 |             | rejected     accepted    plot  output(1/0)  max      |           interval into 'n'   outside the station")
   print ("                  |       |       |    |                |                 |             |   PSDs(1/0)   PSDs(1/0)  PDFs(1/0) |         |       |              | segments       noise baseline")
   print ("                  |       |       |    |                |                 |             |       |         |        |         |         |       |              |                  |")
   print ("                  |       |       |    |                |                 |             |       |         |        |         |         |       |              |                  |      compute H/V using method (see below)")
   print ("                  |       |       |    |                |                 |             |       |         |        |         |         |       |              |                  |        |")
   print ("   computeHVSR.py net=TA sta=TCOL loc= chan=BHZ,BHN,BHE start=2013-01-01 end=2013-01-01 plot=1 plotbad=0 plotpsd=0 plotpdf=1 verbose=1 ymax=5 xtype=frequency n=1 removeoutliers=1 method=4\n")
   print ("       method:")
   print ("             (1) DFA, Diffuse Field Assumption method (Sánchez-Sesma et al., 2011)")
   print ("                 NOTE: The MUSTANG noise-psd web service Power Spectral Density estimate for seismic channels are computed using the algorithm outlined here")
   print ("                       (http://service.iris.edu/mustang/noise-psd/docs/1/help/). This algorithm involves averaging and normalization that may result in")
   print ("                       smoothing of some of the peaks that may otherwise be observed by direct computation of FFT and DFA. With this smoothing the DFA")
   print ("                       results tend to be closer to the vector summation method, method (4) below.\n")
   print ("             NOTE: methods 2-6 are referenced by Albarello and Lunedei (2013)")
   print ("             (2) arithmetic mean, H ≡ (HN + HE)/2")
   print ("             (3) geometric mean, H ≡ √HN · HE")
   print ("             (4) vector summation, H ≡ √H2 N + H2 E")
   print ("             (5) quadratic mean, H ≡ √(H2 N + H2 E )/2")
   print ("             (6) maximum horizontal value, H ≡ max {HN, HE}\n\n")
   print ("Albarello, Dario & Lunedei, Enrico. (2013). Combining horizontal ambient vibration components for H/V spectral ratio estimates. Geophysical Journal International. 194. 936-951. 10.1093/gji/ggt130.")
   print ("Francisco J Sánchez-Sesma, Francisco & Rodriguez, Miguel & Iturraran-Viveros, Ursula & Luzón, Francisco & Campillo, Michel & Margerin, Ludovic & García-Jerez, Antonio & Suarez, Martha & Santoyo, Miguel &\n")
   print ("Rodríguez-Castellanos, A. (2011). A theory for microtremor H/V spectral ratio: Application for a layered medium. Geophysical Journal International. 186. 221-225. 10.1111/j.1365-246X.2011.05064.x.\n")
   print ("Guidelines for the Implementation of the H/V Spectral Ratio Technique on Ambient Vibrations, December 2004")
   print ("SESAME European research project WP12 – Deliverable D23.12, European Commission – Research General Directorate")
   print ("Project No. EVG1-CT-2000-00026 SESAME.")
   print ("ftp://ftp.geo.uib.no/pub/seismo/SOFTWARE/SESAME/USER-GUIDELINES/SESAME-HV-User-Guidelines.pdf")
 
   print ("\n\n\n")


def timeIt(t0):
   """compute elapsed time since the last call"""
   t1 = time.time()
   dt = t1 - t0
   t = t0
   if dt > 0.05:
     print(" ".join(["[TIME]",str(dt),"seconds"]))
     t = t1
   return t


def dateRange(start, end, intv):
    """break an interval to date ranges
    this is used to avoid large requests that get rejected
    """
    if intv <= 1:
       dateList = [start,end]
    else:
       dateList = []
       from datetime import datetime
       startT = datetime.strptime(start,"%Y-%m-%d")
       endT = datetime.strptime(end,"%Y-%m-%d")
       diff = (endT  - startT ) / intv
       if diff.days <= 1:
           dateList = [start,end]
       else:
          for i in range(intv):
              dateList.append((startT + diff * i).strftime("%Y-%m-%d"))
          dateList.append(endT.strftime("%Y-%m-%d"))
    return dateList


def printPeakReport(stationHeader,reportHeader,peak,reportinfo,minRank):
   """print a report of peak parameters"""
   index = []
   rank  = []

   if reportinfo:
      print("\n\nPeaks:")
      print("Parameters and ranking based on SESAME 2004 (A0: peak amplitude, f0: peak frequency):\n")
      print("- amplitude clarity conditions:")
      print("\t. there exist one frequency f-, lying between f0/4 and f0, such that A0 / A(f-) > 2")
      print("\t. there exist one frequency f+, lying between f0 and 4*f0, such that A0 / A(f+) > 2")
      print("\t. A0 > 2\n")
      print("- amplitude stability conditions:")
      print("\t. peak appear within +/-5% on H/V curves of mean +/- one standard deviation (f0+/f0-)")
      print("\t. "+GREEK_CHAR['sigma']+"f lower than a frequency dependent threshold "+GREEK_CHAR['epsilon']+"(f)")
      print("\t. "+GREEK_CHAR['sigma']+"A lower than a frequency dependent threshold log "+GREEK_CHAR['teta']+"(f)\n")
      #print("\n\n%s"%(reportHeader))
   for i in range(len(peak)):
      index.append(i)
      rank.append(peak[i]['Score'])
   listTmp = list(zip(rank, index))   
   listTmp.sort(reverse=True)

   if reportinfo:
      print("\n%47s %10s %22s %12s %12s %32s %32s %27s %22s %17s"
             %("Net.Sta.Loc.Chan","    f0    ","        A0 > 2        ","     f-      ","    f+     ","     f0- within ±5% of f0 &     ","     f0+ within ±5% of f0       ",GREEK_CHAR['sigma']+"f < "+GREEK_CHAR['epsilon']+" * f0      ",GREEK_CHAR['sigma']+"logH/V < log"+GREEK_CHAR['teta']+"    ","   Score/Max.    "))
      print("%47s %10s %22s %12s %12s %32s %32s %27s %22s %17s\n" 
             %(47 * SEPARATOR_CHAR,10 * SEPARATOR_CHAR,22 * SEPARATOR_CHAR,12 * SEPARATOR_CHAR, 12 * SEPARATOR_CHAR, 32 * SEPARATOR_CHAR,32 * SEPARATOR_CHAR,27 * SEPARATOR_CHAR,22 * SEPARATOR_CHAR,17 * SEPARATOR_CHAR))

   peakVisible = []
   for i in range(len(listTmp)):
      index = listTmp[i][1]
      thisPeak = peak[index]
      if(float(thisPeak['Score']) < minRank):
         continue
      else:
         peakVisible.append(True)
      print("%47s %10.3f %22s %12s %12s %32s %32s %27s %22s %12d/%0d\n"%(stationHeader,thisPeak['f0'],thisPeak['Report']['A0'],thisPeak['f-'],thisPeak['f+'],thisPeak['Report']['P-'],thisPeak['Report']['P+'],thisPeak['Report']['Sf'],thisPeak['Report']['Sa'],thisPeak['Score'],maxRank))
   if len(listTmp) <= 0 or len(peakVisible) <= 0:
       print("%47s\n"%(stationHeader))


def getArgs(argList):
   """get the run arguments"""
   args = {}
   for i in range(1,len(argList)):
      key,value = argList[i].split('=')
      args[key] = value
   return args


def getParam(args,key,msgLib,value,verbose=-1):
   """get a run argument for the given key"""
   if key in args.keys():
      if verbose >= 0: 
         print (key,args[key])
      return args[key]
   elif value is not None:
      return value
   else:
      msgLib.error("missing parameter "+key,1)
      usage()
      sys.exit()


def checkYRange(y,low,high):
   """check the PSD values to see if they are within the range"""
   OK    = []
   NOTOK = []
   #
   # use subtract operator to see if y and low/high are crossing
   #
   for i in range(len(y)):
#      l = list(map(operator.sub,y[i],low))
      l = [a - b for a, b in zip(y[i], low)]
      if min(l) < 0 :
         NOTOK.append(i)
         continue

#      h = list(map(operator.sub,y[i],high))
      h = [a - b for a, b in zip(y[i], high)]
      if max(h) > 0 :
         NOTOK.append(i)
         continue

      OK.append(i)
         
   return (OK,NOTOK)


def removeDb(dbValue):
   """convert dB power to power"""
   values = []
   for d in dbValue:
      values.append(10**(float(d)/10.0))
   return values


def getPower(dB,x):
   """calculate HVSR
      We will undo setp 6 of MUSTANG processing as outlined below:
          1. Dividing the window into 13 segments having 75% overlap
          2. For each segment:
             2.1 Removing the trend and mean
             2.2 Apply a 10% sine taper
             2.3 FFT
          3. Calculate the normalized PSD
          4. Average the 13 PSDs & scale to compensate for tapering
          5. Frequency-smooth the averaged PSD over 1-octave intervals at 1/8-octave increments
          6. Convert power to decibels

    NOTE: PSD is equal to the power divided by the width of the bin
          PSD = P / W
          log(PSD) = Log(P) - log(W)
          log(P)   = log(PSD) + log(W)  here W is width in frequency
          log(P)   = log(PSD) - log(Wt) here Wt is width in period
   
    for each bin perform rectangular integration to compute power
    power is assigned to the point at the begining of the interval
         _   _
        | |_| |
        |_|_|_|
   
    Here we are computing power for individual ponts, so, no integration is necessary, just
    compute area
   """     
   import numpy as np
   dx = np.diff(x)[0]
   p  = np.multiply(np.mean(removeDb(dB)) , dx)
   return p

def getHvsr(dBz,dB1,dB2,x,method=4):
   """
   H is computed based on the selected method see: https://academic.oup.com/gji/article/194/2/936/597415
       method:
          (1) DFA
          (2) arithmetic mean, that is, H ≡ (HN + HE)/2
          (3) geometric mean, that is, H ≡ √HN · HE, recommended by the SESAME project (2004)
          (4) vector summation, that is, H ≡ √H2 N + H2 E 
          (5) quadratic mean, that is, H ≡ √(H2 N + H2 E )/2
          (6) maximum horizontal value, that is, H ≡ max {HN, HE}
   """
   pz = getPower(dBz,x)
   p1 = getPower(dB1,x)
   p2 = getPower(dB2,x)

   hz = math.sqrt(pz)
   h1 = math.sqrt(p1)
   h2 = math.sqrt(p2)

   h  ={2:(h1+h2)/2.0, 3:math.sqrt(h1 * h2), 4:math.sqrt(p1 + p2), 5: math.sqrt((p1 + p2)/2.0), 6: max(h1,h2)}

   hvsr = h[method]/hz
   return hvsr


def findPeaks(Y):
   """find peaks"""
   from scipy.signal import argrelextrema
   indexList = argrelextrema(np.array(Y), np.greater)

   return indexList[0]


def initPeaks(X,Y,indexList,hvsrBand,peakWaterLevel):
   """initialize peaks"""
   peak    = []
   for i in indexList:
      if Y[i] > peakWaterLevel[i] and (X[i] >= hvsrBand[0] and X[i] <= hvsrBand[1]):
         peak.append({'f0':float(X[i]),'A0':float(Y[i]),'f-':None,'f+':None,'Sf':None,'Sa':None,'Score':0,'Report':{'A0':'','Sf':'','Sa':'','P+':'','P-':''}})
   return peak


def checkClarity(X,Y,peak,rank=False):
   """
      test peaks for satisfying amplitude clarity conditions as outlined by SESAME 2004:
          - there exist one frequency f-, lying between f0/4 and f0, such that A0 / A(f-) > 2
          - there exist one frequency f+, lying between f0 and 4*f0, such that A0 / A(f+) > 2
          - A0 > 2
   """
   global maxRank

   # 
   # peaks with A0 > 2
   #
   if rank:
     maxRank += 1
   a0 = 2.0
   for i in range(len(peak)):
     
      if float(peak[i]['A0']) > a0:
         peak[i]['Report']['A0'] = "%10.2f > %0.1f %1s"%(peak[i]['A0'],a0,u'\u2713')
         peak[i]['Score'] += 1
      else:
         peak[i]['Report']['A0'] = "%10.2f > %0.1f  "%(peak[i]['A0'],a0)
   #
   # test each peak for clarity
   #
   if rank:
     maxRank += 1
   for i in range(len(peak)):
      peak[i]['f-'] = '-'
      for j in range(len(X)-1,-1,-1):
         #
         # there exist one frequency f-, lying between f0/4 and f0, such that A0 / A(f-) > 2
         #
         if (X[j] >= float(peak[i]['f0'])/4.0 and X[j] < float(peak[i]['f0'])) and float(peak[i]['A0'])/Y[j] > 2.0:
            peak[i]['f-']     = "%10.3f %1s"%(X[j],u'\u2713')
            peak[i]['Score'] += 1 
            break

   if rank:
      maxRank += 1
   for i in range(len(peak)):
      peak[i]['f+'] = '-'
      for j in range(len(X)-1):
         #
         # there exist one frequency f+, lying between f0 and 4*f0, such that A0 / A(f+) > 2
         #
         if (X[j] <= float(peak[i]['f0'])*4.0 and X[j] > float(peak[i]['f0'])) and float(peak[i]['A0'])/Y[j] > 2.0:
            peak[i]['f+']     = "%10.3f %1s"%(X[j],u'\u2713')
            peak[i]['Score'] += 1 
            break
         
   return peak
   

def checkFreqStability(peak,peakm,peakp):
   """
      test peaks for satisfying stability conditions as outlined by SESAME 2004:
          - the peak should appear at the same frequency (within a percentage ± 5%) on the H/V
            curves corresponding to mean + and – one standard deviation.
   """
   global maxRank

   # 
   # check σf and σA
   #
   maxRank += 1

   foundm = []
   for i in range(len(peak)):
      dx = 1000000.
      foundm.append(False)
      peak[i]['Report']['P-'] = "- &"
      for j in range(len(peakm)):
         if abs(peakm[j]['f0'] - peak[i]['f0']) < dx:
            index = j
            dx = abs(peakm[j]['f0'] - peak[i]['f0'])
         if (peakm[j]['f0'] >= peak[i]['f0'] * 0.95 and peakm[j]['f0'] <= peak[i]['f0']* 1.05):
            peak[i]['Report']['P-'] = "%0.3f within ±5%s of %0.3f %1s"%(peakm[j]['f0'],'%',peak[i]['f0'],'&')
            foundm[i] = True
            break
      if peak[i]['Report']['P-'] == "-":
         peak[i]['Report']['P-'] = "%0.3f within ±5%s of %0.3f %1s"%(peakm[index]['f0'],'%',peak[i]['f0'],'&') 

   foundp = []
   for i in range(len(peak)):
      dx = 1000000.
      foundp.append(False)
      peak[i]['Report']['P+'] = "-"
      for j in range(len(peakp)):
         if abs(peakp[j]['f0'] - peak[i]['f0']) < dx:
            index = j
            dx = abs(peakp[j]['f0'] - peak[i]['f0'])
         if (peakp[j]['f0'] >= peak[i]['f0'] * 0.95 and peakp[j]['f0'] <= peak[i]['f0']* 1.05):
            if foundm[i]:
               peak[i]['Report']['P+'] = "%0.3f within ±5%s of %0.3f %1s"%(peakp[j]['f0'],'%',peak[i]['f0'],u'\u2713')
               peak[i]['Score'] += 1
            else:
               peak[i]['Report']['P+'] = "%0.3f within ±5%s of %0.3f %1s"%(peakp[index]['f0'],'%',peak[i]['f0'],' ') 
            break
      if peak[i]['Report']['P+'] == "-" and len(peakp) > 0:
         peak[i]['Report']['P+'] = "%0.3f within ±5%s of %0.3f %1s"%(peakp[index]['f0'],'%',peak[i]['f0'],' ') 
            
   return peak


def checkStability(stdf,peak,hvsrlogstd=False,rank=False):
   """
   test peaks for satisfying stability conditions as outlined by SESAME 2004:
      - σf lower than a frequency dependent threshold ε(f)
      - σA (f0) lower than a frequency dependent threshold θ(f),
   """

   global maxRank

   # 
   # check σf and σA
   #
   if rank:
      maxRank += 2
   for i in range(len(peak)):
      peak[i]['Sf'] = stdf[i]
      peak[i]['Sa'] = hvsrlogstd[i]
      thisPeak = peak[i]
      if thisPeak['f0'] < 0.2:
         e = 0.25
         if stdf[i] < e * thisPeak['f0']:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f %1s"%(stdf[i],e,thisPeak['f0'],u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f  "%(stdf[i],e,thisPeak['f0'])

         t = 0.48
         if hvsrlogstd[i] < t:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f %1s"%(hvsrlogstd[i],t,u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f  "%(hvsrlogstd[i],t)

      elif thisPeak['f0'] >= 0.2 and thisPeak['f0'] < 0.5:
         e = 0.2
         if stdf[i] < e * thisPeak['f0']:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f %1s"%(stdf[i],e,thisPeak['f0'],u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f  "%(stdf[i],e,thisPeak['f0'])

         t = 0.40
         if hvsrlogstd[i] < t:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f %1s"%(hvsrlogstd[i],t,u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f  "%(hvsrlogstd[i],t)

      elif thisPeak['f0'] >= 0.5 and thisPeak['f0'] < 1.0:
         e = 0.15
         if stdf[i] < e * thisPeak['f0']:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f %1s"%(stdf[i],e,thisPeak['f0'],u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f  "%(stdf[i],e,thisPeak['f0'])

         t = 0.3
         if hvsrlogstd[i] < t:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f %1s"%(hvsrlogstd[i],t,u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f  "%(hvsrlogstd[i],t)

      elif thisPeak['f0'] >= 1.0 and thisPeak['f0'] <= 2.0:
         e = 0.1
         if stdf[i] < e * thisPeak['f0']:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f %1s"%(stdf[i],e,thisPeak['f0'],u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f  "%(stdf[i],e,thisPeak['f0'])
        
         t = 0.25
         if hvsrlogstd[i] < t:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f %1s"%(hvsrlogstd[i],t,u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f  "%(hvsrlogstd[i],t)

      elif thisPeak['f0'] > 0.2:
         e = 0.05
         if stdf[i] < e * thisPeak['f0']:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f %1s"%(stdf[i],e,thisPeak['f0'],u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sf'] = "%10.4f < %0.2f * %0.3f  "%(stdf[i],e,thisPeak['f0'])

         t = 0.2
         if hvsrlogstd[i] < t:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f %1s"%(hvsrlogstd[i],t,u'\u2713')
            thisPeak['Score'] +=1
         else:
            peak[i]['Report']['Sa'] = "%10.4f < %0.2f  "%(hvsrlogstd[i],t)
   return peak         


def getPDF(URL,verbose):
   """get PDF"""
   xValues = []
   yValues = []
   X       = []
   Y       = []
   P       = []

   if verbose >= 0:
      msgLib.info('requesting:'+URL)
   try:
      link    = urllib.request.urlopen(URL)
   except urllib.error.HTTPError as e:
      gotData = False
      print ("\n")
      print(e.code)
      print(e.reason)
      if 'Too Large' in e.reason:
           print('Note: use the run argument "n" to split the requested date range to smaller intervals')
      #print(e.headers)
      msgLib.error("failed on target "+target+" "+URL,1)
      return (X,Y,P)

   if verbose >= 0:
      msgLib.info("PDF waiting for reply....")

   data     = link.read().decode()
   link.close()
   lines    = data.split('\n')
   lastFreq = ''
   gotData  = True
   lineCount        = 0
   nonBlankLastLine = 0
   if len(lines[-1].strip()) <= 0:
      nonBlankLastLine = 1
   for line in lines:
      lineCount += 1
      if len(line.strip()) <= 0:
         continue
      if line[0] == '#' or ',' not in line:
         continue
      (freq, power, hits) = line.split(',')
      if lastFreq == '':
         lastFreq = freq.strip()
         powerList = []
         powerList.append(float(power))
         hitsList  = []
         hitsList.append(int(hits))
      elif lastFreq == freq.strip():
         powerList.append(float(power))
         hitsList.append(int(hits))
      if lastFreq != freq.strip() or lineCount == len(lines) - nonBlankLastLine:
         totalHits=sum(hitsList)
         values = []
         yValues.append(np.array(hitsList)*100.0/totalHits)
         if xtype == "period":
            lastX = 1.0/float(lastFreq)
            xValues.append(lastX)
         else:
            lastX = float(lastFreq)
            xValues.append(lastX)
         for i in range(len(hitsList)):
            Y.append(float(powerList[i]))
            P.append(float(hitsList[i]) * 100.0 / float(totalHits))
            X.append(lastX)

         lastFreq = freq.strip()
         powerList = []
         powerList.append(float(power))
         hitsList  = []
         hitsList.append(int(hits))
   return (X,Y,P)

################################################################################################
#
# Main
#
################################################################################################

#
# set paths
#
import os, sys
hvsrDirectory  = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))

paramPath      = os.path.join(hvsrDirectory, 'param')
libPath        = os.path.join(hvsrDirectory, 'lib')

sys.path.append(paramPath)
sys.path.append(libPath)

import math, time
import urllib
import xml.etree.ElementTree as ET

#
# import the HVSR parameters and libraries
#
import fileLib as fileLib
import msgLib as msgLib
import computeHVSR_param as param

plotRows = 4

#
# set parameters
#
args = getArgs(sys.argv)
if len(args) <= 1:
   usage()
   sys.exit()

verbose = int(getParam(args,'verbose',msgLib,param.verbose))
if verbose >= 0:
   print ("\n[INFO]",sys.argv[0], version)

reportinfo  = int(getParam(args,'reportinfo',msgLib,1,verbose))

# 
# get channels and sort tem
# the reverse sort order makes sure that we always have ?HZ first. Order of horizontals is not important
#
channels    = getParam(args,'chan',msgLib,param.chan)
channelList = sorted(channels.split(','),reverse=True)
if len(channelList) < 3:
   msgLib.error('need 3 channels!',1)
   sys.exit()
sortedChannelList = channelList.copy()
for channel in channelList:
   sortedChannelList[CHANNEL_ORDER[channel[2]]] = channel

#
# this will tell the script if we want to reject suspect PSDs
#
removeOutliers = bool(int(getParam(args,'removeoutliers',msgLib,False)))

print("[INFO] removeOutliers:",removeOutliers)

#
# minimum SESAME 2004 rank to be accepted
#
minrank = 2
minRank        = float(getParam(args,'minrank',msgLib,param.minrank))
network        = getParam(args,'net',msgLib,None)
if network is None:
   msgLib.error('network not defined!',1)
   sys.exit()
station        = getParam(args,'sta',msgLib,None)
if station is None:
   msgLib.error('station not defined!',1)
   sys.exit()
location       = getParam(args,'loc',msgLib,None)
if location is None:
   msgLib.error('location not defined!',1)
   sys.exit()

start           = getParam(args,'start',msgLib,None)
startHour       = "T00:00:00"
startTime       = time.strptime(start, "%Y-%m-%d")
end             = getParam(args,'end',msgLib,None)
endHour         = "T00:00:00"
endTime         = time.strptime(end, "%Y-%m-%d")

#
# break the start-end interval to n segments
#
n               = int(getParam(args,'n',msgLib,1))
dateList        = dateRange(start,end,n)
print("[INFO] DATE LIST:",dateList)

#
# how to combine h1 & h2
#
method          = int(getParam(args,'method',msgLib,param.method))
if method <= 0 or method > 6:
   msgLib.error('method '+str(method)+' for combining H1 & H2 is invalid!',1)
   sys.exit()
elif method == 1:
   dfa = 1
else:
   dfa = 0

print("[INFO] Using",param.methodList[method])
doPlot          = int(getParam(args,'plot',msgLib,param.plot))
plotPSD         = int(getParam(args,'plotpsd',msgLib,param.plotpsd))
plotPDF         = int(getParam(args,'plotpdf',msgLib,param.plotpdf))
plotBad         = int(getParam(args,'plotbad',msgLib,param.plotbad))
dayValuesPassed = [[],[],[]]
waterLevel      = float(getParam(args,'waterlevel',msgLib,param.waterlevel))
hvsrYlim        = param.hvsrylim
hvsrYlim[1]     = float(getParam(args,'ymax',msgLib,param.hvsrylim[1]))
xtype           = getParam(args,'xtype',msgLib,param.xtype)
hvsrBand        = getParam(args,'hvsrband',msgLib,param.hvsrband)
reportHeader    = '.'.join([network,station,location,'-'.join(sortedChannelList)])
stationHeader   = reportHeader
stationHeader   = ' '.join([stationHeader,start,end])
reportHeader   += ' '
reportHeader   += ' '.join(['from',start,'to',end])
plotTitle       = reportHeader
reportHeader   += '\n\n'


#
# turn off the display requirement if not needed
#
import matplotlib
if doPlot <= 0:
   if verbose >= 0:
      print ("[INFO] PLOT OFF")
   matplotlib.use('agg')
else:
   from obspy.imaging.cm import pqlx

#
# one channel at a time
#
channelIndex = -1
for channel in sortedChannelList:
   channelIndex += 1
   xValues       = []
   psdValues     = []
   dayValues     = []
   dayTimeValues = []
   pctLow        = []
   pctHigh       = []
   pctMid        = []

   target        = '.'.join([network,station,location,channel,"*"])
   label         = '.'.join([network,station,location,'PSDs'])
   labelHVSR     = '.'.join([network,station,location,'HVSR'])
   if verbose >= 0 :
      msgLib.info("requesting "+target+" from "+start+" to "+end)

   #
   # baseline files are required if we will remove the outliers
   # we assume the baseline file has all the periods, so we use it as a reference
   #
   if removeOutliers:
      try:
         baselineFile = open(os.path.join(param.baselineDirectory,fileLib.baselineFileName(network,station,location,channel)),'r')
      except:
         msgLib.error("failed to read baseline file "+os.path.join(param.baselineDirectory,fileLib.baselineFileName(network,station,location,channel)),1)
         sys.exit()

      lines = baselineFile.read()
      baseline = lines.split('\n')
      for indexValue in range(0,len(baseline)):
         if len(baseline[indexValue].strip()) == 0:
            continue
         if baseline[indexValue].strip().startswith('#'):
            values = baseline[indexValue].strip().split()
            percentLow  = values[1]
            percentMid  = values[3]
            percentHigh = values[5]
            continue

         values = baseline[indexValue].split()
     
         xValues.append(float(values[0]))
         pctLow.append(float(values[1]))
         pctMid.append(float(values[2]))
         pctHigh.append(float(values[3]))
      baselineFile.close()

   #
   # get daily PSDs from MUSTANG
   # Limit PSD segments starting between starttime (inclusive) and endtime (exclusive)
   # 
   pdfX = []
   pdfY = []
   pdfP = []
   for dateIndex in range(len(dateList)-1):
      print("[INFO] doing ",dateList[dateIndex]+startHour,'to',dateList[dateIndex+1]+endHour)
      URL = param.mustangPsdUrl+'target='+target+'&starttime='+dateList[dateIndex]+startHour+'&endtime='+dateList[dateIndex+1]+endHour+'&format=xml&correct=true'
      if verbose >= 0:
         msgLib.info('requesting:'+URL)
         t0 = timeIt(t0)
      try:
         link    = urllib.request.urlopen(URL)
      except urllib.error.HTTPError as e:
         if verbose >= 0:
            print (e.code)
            print(e.reason)
            if 'Too Large' in e.reason:
              print('Note: use the run argument "n" to split the requested date range to smaller intervals')
              print('Current "n" value is:',n)
            msgLib.error("failed on target "+target+" "+URL,1)
         continue

      if verbose:
         msgLib.info("PSD waiting for reply....")

      tree = ET.parse(link)
      link.close()
      root = tree.getroot()

      if verbose:
         msgLib.info(root.tag)
         requestStart = root.find('RequestedDateRange').find('Start').text
         requestEnd   = root.find('RequestedDateRange').find('End').text

      psds = root.find('Psds')

      allPsds = psds.findall('Psd')
      if verbose:
         msgLib.info("PSD: "+str(len(allPsds)))
         t0 = timeIt(t0)

      for psd in allPsds:
         day     = psd.attrib['start'].split('T')[0]
         psdTime = time.strptime(day, "%Y-%m-%d")
         if psdTime < startTime or psdTime >= endTime:
            if verbose >= 0:
               msgLib.warning(sys.argv[0],"rejected PSD of "+psd.attrib['start'])
            continue
         allValues = psd.findall('value')
         
         X = []
         Y = []
         for value in allValues:
            X.append(float(value.attrib['freq']))
            Y.append(float(value.attrib['power']))

         #
         # we follow a simple logic, the X values must match. We take the first one to be the sequence we want
         #
         if not xValues:
            xValues = list(X)

         if X != xValues:
            if verbose:
               msgLib.warning(sys.argv[0]," ".join(["rejected",target,dateList[dateIndex],dateList[dateIndex+1],"for bad X"]))
         else:
            #
            # store the PSD values and at the same time keep track of their day and time
            #
            dayValues.append(day)
            dayTimeValues.append(psd.attrib['start'])
            psdValues.append(Y)

      if plotPDF:
         (thisX,thisY,thisP) = getPDF(param.mustangPdfUrl+'target='+target+'&starttime='+dateList[dateIndex]+'&endtime='+dateList[dateIndex+1]+'&format=text',verbose)
         pdfX += thisX
         pdfY += thisY
         pdfP += thisP
         if verbose:
            msgLib.info("PDF: "+str(len(pdfY)))
   #
   # must have PSDs
   #
   if not psdValues:
      if verbose >= 0:
         msgLib.error(" ".join(["no PSDs found",dateList[dateIndex],dateList[dateIndex+1]]),1)
      sys.exit()
   else:
      if verbose >= 0:
         msgLib.info("total PSDs:"+str(len(psdValues)))
         t0 = timeIt(t0)

   #
   # work on PSDs
   # 

   #
   # initial settings
   #
   if channelIndex == 0:
      if doPlot:
         if verbose >= 0:
            msgLib.info("[INFO] PLOT PSD")
         import matplotlib.pyplot as plt
         from matplotlib.offsetbox import AnchoredText
         fig = plt.figure(figsize=(param.imageSize),facecolor="white")
         ax  = []
         fig.canvas.set_window_title(label)
         ax.append(plt.subplot(plotRows,1,channelIndex+1))

      #
      # [chanZ[day],chan1[day],chan2[day]]
      #
      dailyPSD          = [{},{},{}]
      dayTimePSD        = [{},{},{}]
      medianDailyPSD    = [{},{},{}]
      equalDailyEnergy  = [{},{},{}]
   else:
      if doPlot:
         ax.append(plt.subplot(plotRows,1,channelIndex+1, sharex=ax[0]))

   #
   # go through all PSDs and reject the "bad" ones based on the station baseline
   # only done when removeOutliers is True
   #
   if removeOutliers:
      if verbose:
         msgLib.info("CLEAN UP "+str(len(psdValues))+" PSDs")
      (ok,notok) = checkYRange(psdValues,pctLow,pctHigh)
   else:
      #
      # no cleanup needed, mark them all as OK
      #
      notok = []
      ok    = range(len(psdValues))

   info          = ' '.join(['Channel',channel,str(len(psdValues)),"PSDs,",str(len(ok)),'accepted and',str(len(notok)),'rejected','\n'])
   reportHeader += info
   print ("[INFO]:",info)
   
   if verbose and notok:
      t0 = timeIt(t0)
      msgLib.info("FLAG BAD PSDs")
   for i in range(len(ok)):
         index   = ok[i]
         # DAY,DAYTIME: 2018-01-01 2018-01-01T00:00:00.000Z
         day     = dayValues[index]
         dayTime = dayTimeValues[index]
         psd     = psdValues[index]
   
         #
         # preserve the individual PSDs (dayTime)
         #
         dayTimePSD[channelIndex][dayTime] = psd

         #
         # group PSDs into daily bins
         #
         if day not in dailyPSD[channelIndex].keys():
            dailyPSD[channelIndex][day] = []
         dailyPSD[channelIndex][day].append(psd)

         #
         # keep track of individual days
         #
         if dayValues[index] not in dayValuesPassed[channelIndex]:
            dayValuesPassed[channelIndex].append(day)
   if verbose and notok:
      t0 = timeIt(t0)

   #
   # plot the "bad" PSDs in gray
   #
   if doPlot:
      if plotBad:
         msgLib.info("[INFO] PLOT BAD PSDs")
         for i in range(len(notok)):
            plt.semilogx(np.array(xValues), psdValues[notok[i]], c='gray')
         if verbose >= 0:
            t0 = timeIt(t0)

      if plotPSD:
         #
         # plot the "good" PSDs in green
         #
         if verbose:
            msgLib.info("[INFO] PLOT GOOD PSDs")
         for i in range(len(ok)):
            plt.semilogx(np.array(xValues), psdValues[ok[i]], c='green')
         if verbose >= 0:
            t0 = timeIt(t0)
      elif plotPDF:
         # define the colormap
         #cmap = plt.cm.jet
         # extract all colors from the .jet map
         #cmaplist = [cmap(i) for i in range(cmap.N)]
         # force the first color entry to be grey
         #cmaplist[0] = "mediumorchid"
         # create the new map
         #cmap = cmap.from_list('Custom cmap', cmaplist, cmap.N)
         cmap = pqlx

         ok      = []
         im = plt.scatter(pdfX,pdfY,c=pdfP, s=46.5, marker = '_',  linewidth=param.lw, edgecolor='face', cmap=cmap, alpha=param.alpha)
         ax[channelIndex].set_xscale('log')

      if verbose:
         msgLib.info("[INFO] TUNE PLOTS")
      if verbose >= 0:
         t0 = timeIt(t0)
      if removeOutliers:
         plt.semilogx(np.array(xValues),pctHigh,c='yellow',label=str(percentHigh)+'%')
         plt.semilogx(np.array(xValues),pctMid,c='red',label=str(percentMid)+'%')
         plt.semilogx(np.array(xValues),pctLow,c='orange',label=str(percentLow)+'%')
      plt.semilogx((param.hvsrXlim[0],param.hvsrXlim[0]),param.yLim,c='black')
      plt.semilogx((param.hvsrXlim[1],param.hvsrXlim[1]),param.yLim,c='black')
      p1 = plt.axvspan(param.xLim[xtype][0], param.hvsrXlim[0], facecolor='#909090', alpha=0.5)
      p2 = plt.axvspan(param.hvsrXlim[1], param.xLim[xtype][1], facecolor='#909090', alpha=0.5)
      #plt.title(" ".join([target,start,'to',end]))
      plt.ylim(param.yLim)
      plt.xlim(param.xLim[xtype])
      plt.ylabel(param.yLabel)
      plt.legend(loc='lower left')
      if len(ok) <= 0:
         anchored_text = AnchoredText(' '.join(['.'.join([network,station,location,channel]),"{:,d}".format(len(psdValues)),'PSDs']), loc=2)
      else:
         anchored_text = AnchoredText(' '.join(['.'.join([network,station,location,channel]),"{:,d}".format(len(ok)),'out of',"{:,d}".format(len(psdValues)),'PSDs']), loc=2)
      ax[channelIndex].add_artist(anchored_text)

      # create a second axes for the colorbar
      if plotPDF:
         ax2 = fig.add_axes([0.92, 0.1, 0.01, 0.8])
         fig.colorbar(im, ax2, orientation='vertical')
         ax2.set_ylabel('Probability (%)', size=9, rotation=270, labelpad=7)
         plt.clim(param.pMin,param.pMax)

   #
   # compute and save the median daily PSD for HVSR computation
   # for non-DFA computation
   #
   if not dfa:
      if verbose:
         msgLib.info("SAVE MEDIAN DAILY")
      for day in (dayValuesPassed[channelIndex]):
          if display:
             print("[INFO] calculating medianDailyPSD")
             display = False 
          medianDailyPSD[channelIndex][day] = np.percentile(dailyPSD[channelIndex][day],50,axis=0)

#
# are we doing DFA?
# use equal energy for daily PSDs to give small "events" a chance to contribute 
# the same as large ones, so that P1+P2+P3=1
#
if dfa:
   if display:
      print("[INFO] DFA")
      display = False
   sumNSPower = []
   sumEWPower = []
   sumZPower  = []
   dailyPSD   = [{},{},{}]
   dayValues  = []

   #
   # make sure we have all 3 components for every time sample
   #
   for dayTime in(dayTimeValues):
      if dayTime not in (dayTimePSD[0].keys()) or dayTime not in (dayTimePSD[1].keys()) or dayTime not in (dayTimePSD[2].keys()):
         continue
      day = dayTime.split('T')[0]
      if day not in dayValues:
         dayValues.append(day)

      #
      # initialize the daily PSDs
      #
      if day not in dailyPSD[0].keys():
         dailyPSD[0][day] = []
         dailyPSD[1][day] = []
         dailyPSD[2][day] = []

      dailyPSD[0][day].append(dayTimePSD[0][dayTime])
      dailyPSD[1][day].append(dayTimePSD[1][dayTime])
      dailyPSD[2][day].append(dayTimePSD[2][dayTime])

   #
   # for each day equalize energy
   #
   for day in dayValues:

      #
      # each PSD for the day
      #
      for i in range(len(dailyPSD[0][day])):
         Pz    = []
         P1    = []
         P2    = []
         sumPz = 0
         sumP1 = 0
         sumP2 = 0

         #
         # each sample of the PSD , convert to power
         #
         for j in range(len(xValues)-1):
            pz     = getPower([dailyPSD[0][day][i][j],dailyPSD[0][day][i][j+1]],[xValues[j],xValues[j+1]])
            Pz.append(pz)
            sumPz += pz
            p1     = getPower([dailyPSD[1][day][i][j],dailyPSD[1][day][i][j+1]],[xValues[j],xValues[j+1]])
            P1.append(p1)
            sumP1 += p1
            p2     = getPower([dailyPSD[2][day][i][j],dailyPSD[2][day][i][j+1]],[xValues[j],xValues[j+1]])
            P2.append(p2)
            sumP2 += p2

         sumPower = sumPz + sumP1 + sumP2 # total power

         #
         # normalized power
         #
         for j in range(len(xValues)-1):
            #
            # initialize if this is the first sample of the day
            #
            if i == 0:
               sumZPower.append(Pz[j] / sumPower)
               sumNSPower.append(P1[j] / sumPower)
               sumEWPower.append(P2[j] / sumPower)
            else:
               sumZPower[j]  += (Pz[j] / sumPower)
               sumNSPower[j] += (P1[j] / sumPower)
               sumEWPower[j] += (P2[j] / sumPower)
      #
      # average the normalized daily power
      #
      for j in range(len(xValues)-1):
         sumZPower[j]  /= len(dailyPSD[0][day])
         sumNSPower[j] /= len(dailyPSD[0][day])
         sumEWPower[j] /= len(dailyPSD[0][day])

      equalDailyEnergy[0][day] = sumZPower.copy()
      equalDailyEnergy[1][day] = sumNSPower.copy()
      equalDailyEnergy[2][day] = sumEWPower.copy()

#
# HVSR computation
#
if verbose:
   msgLib.info("HVSR computation")
if verbose >= 0:
   t0 = timeIt(t0)

#
# find the unique days between all channels
#
d = dayValuesPassed[0]
for i in range(1,len(dayValuesPassed)):
   d += dayValuesPassed[i]
dayValuesPassed = set(d) # unique days

hvsr            = []
peakWaterLevel  = []
hvsrp           = []
peakWaterLevelp = []
hvsrp2          = []
hvsrm           = []
waterLevelm     = []
peakWaterLevelm = []
hvsrm2          = []
hvsrstd         = []
hvsrlogstd      = []

path   = "".join(["M",str(method)])
fileLib.mkdir(param.hvsrDirectory,path)
outFileName = os.path.join (param.hvsrDirectory,path, fileLib.hvsrFileName(network,station,location,start,end))

outFile         = open (outFileName,"w")
print("OUT FILE:",outFileName)
count           = -1

#
# compute one x-value (period or frequency) at a time to also compute standard deviation
#
outFile.write("frequency HVSR HVSR+1STD HVSR-1STD\n")
for j in range(len(xValues)-1):
   missing = 0
   hvsrTmp = []

   for day in sorted(dayValuesPassed):
      #
      # must have all 3 channels, compute HVSR for that day
      #
      if dfa:
         if day in equalDailyEnergy[0].keys() and day in equalDailyEnergy[1].keys() and day in equalDailyEnergy[2].keys():
            hvsr0 = math.sqrt((equalDailyEnergy[1][day][j] + equalDailyEnergy[2][day][j]) / equalDailyEnergy[0][day][j])
            hvsrTmp.append(hvsr0)
         else:
            if verbose > 0:
               msgLib.warning(sys.argv[0], day+" missing component, skipped!")
            missing += 1
            continue
      else:
         if day in medianDailyPSD[0].keys() and day in medianDailyPSD[1].keys() and day in medianDailyPSD[2].keys():
            PSD0  = [medianDailyPSD[0][day][j],medianDailyPSD[0][day][j+1]]
            PSD1  = [medianDailyPSD[1][day][j],medianDailyPSD[1][day][j+1]]
            PSD2  = [medianDailyPSD[2][day][j],medianDailyPSD[2][day][j+1]]
            hvsr0 = getHvsr(PSD0,PSD1,PSD2,[xValues[j],xValues[j+1]],method=method)
            hvsrTmp.append(hvsr0)
         else:
            if verbose > 0:
               msgLib.warning(sys.argv[0], day+" missing component, skipped!")
            missing += 1
            continue
   count +=1
   peakWaterLevel.append(waterLevel)
   if len(hvsrTmp) > 0:
      hvsr.append(np.mean(hvsrTmp))
      hvsrstd.append(np.std(hvsrTmp))
      hvsrlogstd.append(np.std(np.log10(hvsrTmp)))
      hvsrp.append(hvsr[-1] + hvsrstd[-1])
      peakWaterLevelp.append(waterLevel + hvsrstd[-1])
      hvsrp2.append(hvsr[-1] * math.exp(hvsrlogstd[count]))
      hvsrm.append(hvsr[-1] - hvsrstd[-1])
      peakWaterLevelm.append(waterLevel - hvsrstd[-1])
      hvsrm2.append(hvsr[-1] / math.exp(hvsrlogstd[-1]))
      #outFile.write(str(xValues[count])+"  "+str(hvsr[-1])+"  "+str(hvsrp[-1])+"  "+str(hvsrm[-1])+"\n")
      outFile.write("%s %0.3f %0.3f %0.3f\n"%(str(xValues[count]),float(hvsr[-1]),float(hvsrp[-1]),float(str(hvsrm[-1]))))
outFile.close()

#
# compute day at a time to also compute frequency standard deviation
#
missing = 0
hvsrPeaks   = [] # this holds the peaks for individual HVSRs that will contribute to the final HVSR. It will be used to find Sigmaf

for day in sorted(dayValuesPassed):
   hvsrTmp = []
   for j in range(len(xValues)-1):
      if dfa > 0:
         if day in equalDailyEnergy[0].keys() and day in equalDailyEnergy[1].keys() and day in equalDailyEnergy[2].keys():
            hvsr0 = math.sqrt((equalDailyEnergy[1][day][j] + equalDailyEnergy[2][day][j]) / equalDailyEnergy[0][day][j])
            hvsrTmp.append(hvsr0)
         else:
            if verbose > 0:
               msgLib.warning(sys.argv[0], day+" missing component, skipped!")
            missing += 1
            continue
      else:
         if day in medianDailyPSD[0].keys() and day in medianDailyPSD[1].keys() and day in medianDailyPSD[2].keys():
            PSD0  = [medianDailyPSD[0][day][j],medianDailyPSD[0][day][j+1]]
            PSD1  = [medianDailyPSD[1][day][j],medianDailyPSD[1][day][j+1]]
            PSD2  = [medianDailyPSD[2][day][j],medianDailyPSD[2][day][j+1]]
            hvsr0 = getHvsr(PSD0,PSD1,PSD2,[xValues[j],xValues[j+1]],method=method)
            hvsrTmp.append(hvsr0)
         else:
            if verbose > 0:
               msgLib.warning(sys.argv[0], day+" missing component, skipped!")
            missing += 1
            continue
   if not np.isnan(np.sum(hvsrTmp)):
      hvsrPeaks.append(findPeaks(hvsrTmp)) 

reportHeader += '\n'
reportHeader += ' '.join([str(missing),'PSDs are missing one or more components\n'])

#
# find  the relative extrema of hvsr
#
if not np.isnan(np.sum(hvsr)):
   indexList = findPeaks(hvsr)
else:
   indexList = []

stdf = []
for index in indexList:
   point = []
   for j in range(len(hvsrPeaks)):
      p = None
      for k in range(len(hvsrPeaks[j])):
         if p is None:
            p = hvsrPeaks[j][k]
         else:
            if abs(index-hvsrPeaks[j][k]) < abs(index-p):
               p = hvsrPeaks[j][k]
      if p is not None:
         point.append(p)
   point.append(index)
   v = []
   for l in range(len(point)):
      v.append(xValues[point[l]])
   stdf.append(np.std(v))
         
peak  = initPeaks(xValues,hvsr,indexList,hvsrBand,peakWaterLevel)
peak  = checkClarity(xValues,hvsr,peak,True)

#
# find  the relative extrema of hvsrp (hvsr + 1 standard deviation)
#
if not np.isnan(np.sum(hvsrp)):
   indexp = findPeaks(hvsrp)
else:
   indexp = []

peakp  = initPeaks(xValues,hvsrp,indexp,hvsrBand,peakWaterLevelp)
peakp  = checkClarity(xValues,hvsrp,peakp)

#
# find  the relative extrema of hvsrp (hvsr - 1 standard deviation)
#
if not np.isnan(np.sum(hvsrm)):
   indexm = findPeaks(hvsrm)
else:
   indexm = []

peakm  = initPeaks(xValues,hvsrm,indexm,hvsrBand,peakWaterLevelm)
peakm  = checkClarity(xValues,hvsrm,peakm)

peak   = checkStability(stdf,peak,hvsrlogstd,True)
peak   = checkFreqStability(peak,peakm,peakp)


if doPlot > 0 and len(hvsr)>0:
   nx = len(xValues) -1
   plt.suptitle(plotTitle)
   ax.append(plt.subplot(plotRows,1,4))
   plt.semilogx(np.array(xValues[0:nx]),hvsr,lw=1,c='b')
   plt.semilogx(np.array(xValues[0:nx]),hvsrp,c='r',lw=1,ls='--')
   plt.semilogx(np.array(xValues[0:nx]),hvsrm,c='r',lw=1,ls='--')
   #plt.semilogx(np.array(xValues),hvsrp2,c='r',lw=1,ls='--')
   #plt.semilogx(np.array(xValues),hvsrm2,c='r',lw=1,ls='--')
   plt.ylabel(param.hvsrYlabel)

   plt.xlim(param.hvsrXlim)
   ax[-1].set_ylim(hvsrYlim)
   plt.xlabel(param.xLabel[xtype])

   for i in range(len(peak)):
      plt.semilogx(peak[i]['f0'],peak[i]['A0'],marker='o',c='r')
      plt.semilogx((peak[i]['f0'],peak[i]['f0']),(hvsrYlim[0],peak[i]['A0']),c='red')
      if stdf[i] < float(peak[i]['f0']) :
         dz = stdf[i]
         plt.axvspan(float(peak[i]['f0']) - dz,float(peak[i]['f0']) + dz, facecolor='#909090', alpha=0.5)
         plt.semilogx((peak[i]['f0'],peak[i]['f0']),(hvsrYlim[0],hvsrYlim[1]),c='#dcdcdc',lw=0.5)

   plt.savefig(os.path.join(param.imageDirectory+"/"+fileLib.hvsrFileName(network,station,location,start,end)).replace(".txt",".png"),dpi=param.imageDpi)
if not dfa:
   printPeakReport(stationHeader,reportHeader,peak,reportinfo,minRank)

if doPlot > 0:
   if verbose >= 0:
      print ("SHOW")
   plt.show()

